
# Syllabus

### Week 1 

#### March 28 

- Get your development environment set up. Read the instructions here and make sure you have the following done:
  + Python 3.5 (Anaconda recommended) installed
  + A Github account
  + A Github repository named `cj2016`, i.e. you should have a Github URL that looks like:
   
        https://github.com/whatevyourname/cj-2016


  + A text editor, such as Sublime Text 3 or Atom.


##### Homework

- In your cj-2016 Github repo, create a new folder named: __week-01__
- In that folder, create a file named __data-writeup.md__
  - [Using the language of Markdown](https://guides.github.com/features/mastering-markdown/), write a short report about your experience with these data portals:

  - http://www.data.gov/
  - http://www.opendatanetwork.com/
  - http://data.cityofpaloalto.org/home
  - http://www.fec.gov/data/DataCatalog.do?format=html
  - https://data.gov.uk/

  For each site, describe what you liked, what you didn't like. What you thought one site did better/worse than the others. Use all the formatting niceties you would use in Microsoft Word (or whatever), [including screenshots, headings, links, etc](https://guides.github.com/features/mastering-markdown/). This is both an exercise in learning about data portals but also publishing on Github.

- Using the data and documentation found here at the NYPD's site:

  http://www.nyc.gov/html/nypd/html/analysis_and_planning/stop_question_and_frisk_report.shtml

  Describe how you would find the record corresponding to the stop-and-frisk of [Giancarlo Esposito](http://www.thewrap.com/tv/article/breaking-bads-giancarlo-esposito-and-healing-power-gus-fring-44131?page=0,1). If it's not obvious, feel free to Google around for other information sources about NYPD's stop and frisk.


#### March 30


- Practice examining NYPD Stop and Frisk Data using interactive Python.
- We'll create our very first news app, courtesy of [NICAR's First News App tutorial](http://first-news-app.readthedocs.org/en/latest/). 



### Week 2 - Text and Visualizing Text

#### April 4 / April 6

We'll continue reviewing the Python programming fundamentals, in the service of deserializing text into data structures, and, when necessary, turning data structures into text files, particularly formatted as CSV and JSON.

We'll also create our first static charts using [matplotlib](http://www.labri.fr/perso/nrougier/teaching/matplotlib/matplotlib.html).

### Week 3 - Filtering noise / Web scraping

#### April 11 / April 13


Our problem is not lack of information. It's lack of attention span. Data is not much good to us if we can't sort it the way we need it to be sorted. Hence, the need to scrape webpages and PDFs.

We'll use <a href="https://projects.propublica.org/docdollars/">ProPublica's Dollars for Docs as a case study</a>.

By now, we'll have written a fair amount of HTML. Web-scraping generally involves learning one more kind of text parser, such as lxml or BeautifulSoup, and writing the automated logic to navigate a website.




### Week 4 - APIs

#### April 18 / April 20

David Yanofsky of Quartz will talk about his entrepreneurial work in data visualization and investigations.







<!-- ### Week 5 

#### April 25 / April 27


### Week 6 

#### May 2 / May 4


### Week 7 

#### May 9 / May 11


### Week 8 

#### May 16 / May 18


### Week 9 

#### May 23 / May 25


### Week 10 

#### May 30 / June 1
 -->
